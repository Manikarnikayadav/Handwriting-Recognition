{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363e714f-a559-4867-95a8-68bff026a7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCAL ENVIRONMENT SETUP \n",
    "!pip install torch torchvision torchaudio --quiet\n",
    "!pip install pytorch-lightning albumentations scipy scikit-image matplotlib tqdm textdistance --quiet\n",
    "\n",
    "import torch, cv2, numpy as np, textdistance\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"OpenCV version:\", cv2.__version__)\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"TextDistance version:\", textdistance.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620f4c6c-399e-4921-afcf-ea7f76f64fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, cv2, numpy as np, textdistance\n",
    "\n",
    "# Check GPU availability\n",
    "print(\"GPU Available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# Test OpenCV image loading\n",
    "import os\n",
    "print(\"OpenCV test:\")\n",
    "blank = np.zeros((50, 100, 3), dtype=np.uint8)\n",
    "cv2.putText(blank, \"OK\", (10, 35), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)\n",
    "cv2.imwrite(\"cv_test.jpg\", blank)\n",
    "img = cv2.imread(\"cv_test.jpg\")\n",
    "print(\"Image shape:\", img.shape)\n",
    "\n",
    "# Test TextDistance Levenshtein distance\n",
    "d = textdistance.levenshtein.distance(\"hello\", \"hallo\")\n",
    "print(\"TextDistance test: 'hello' vs 'hallo' =\", d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79a5f8b-3009-46fc-aba3-35a4581c8bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCAL DATASET EXTRACTION\n",
    "import os, zipfile\n",
    "\n",
    "# Base path \n",
    "BASE_DIR = r\"Your Path\"\n",
    "\n",
    "# Path to ZIP dataset\n",
    "ZIP_PATH = os.path.join(BASE_DIR, \"dataset\", \"archive.zip\")\n",
    "\n",
    "# Where to extract the dataset\n",
    "DATA_ROOT = os.path.join(BASE_DIR, \"dataset\", \"extracted\")\n",
    "\n",
    "print(\"Using local dataset ZIP:\", ZIP_PATH)\n",
    "\n",
    "# Create extraction folder if not exists\n",
    "os.makedirs(DATA_ROOT, exist_ok=True)\n",
    "\n",
    "# Extract only if not already extracted\n",
    "if not any(os.scandir(DATA_ROOT)):\n",
    "    print(\"Extracting dataset... please wait.\")\n",
    "    with zipfile.ZipFile(ZIP_PATH, 'r') as z:\n",
    "        z.extractall(DATA_ROOT)\n",
    "    print(\"Extracted to:\", DATA_ROOT)\n",
    "else:\n",
    "    print(\"Dataset already extracted at:\", DATA_ROOT)\n",
    "\n",
    "# Define type of dataset youâ€™re using\n",
    "DATA_TYPE = \"line\" \n",
    "\n",
    "# Verify contents\n",
    "print(\"\\nTop-level structure:\")\n",
    "for root, dirs, files in os.walk(DATA_ROOT):\n",
    "    print(f\"{root} | subfolders: {dirs[:3]} | #files: {len(files)}\")\n",
    "    break\n",
    "\n",
    "print(\"\\nDATA_ROOT:\", DATA_ROOT)\n",
    "print(\"DATA_TYPE:\", DATA_TYPE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e9ea57-810c-4a6d-9f7e-b512bb75b6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 â€” Inspect Local Dataset Structure\n",
    "import os, glob, pprint\n",
    "\n",
    "# Ensure paths from Cell 2 exist\n",
    "if 'DATA_ROOT' not in globals():\n",
    "    DATA_ROOT = r\"E:\\PYTHON\\CV_Project2\\dataset\\extracted\"   # fallback if not defined\n",
    "\n",
    "if 'DATA_TYPE' not in globals():\n",
    "    DATA_TYPE = \"line\"   \n",
    "\n",
    "print(\"DATA_ROOT:\", DATA_ROOT)\n",
    "print(\"DATA_TYPE:\", DATA_TYPE)\n",
    "\n",
    "print(\"\\nTop-level tree:\")\n",
    "for root, dirs, files in os.walk(DATA_ROOT):\n",
    "    print(f\"{root}   subdirs: {dirs[:5]}   #files: {len(files)}\")\n",
    "    break  \n",
    "\n",
    "# Locate the main dataset directories\n",
    "images_dirs = glob.glob(os.path.join(DATA_ROOT, \"**\", \"Images\"), recursive=True)\n",
    "trans_dirs = glob.glob(os.path.join(DATA_ROOT, \"**\", \"Transcriptions\"), recursive=True)\n",
    "sets_dirs = glob.glob(os.path.join(DATA_ROOT, \"**\", \"Sets\"), recursive=True)\n",
    "\n",
    "print(\"\\nDetected nodes:\")\n",
    "print(\" Images dirs:\", images_dirs[:3])\n",
    "print(\" Transcription dirs:\", trans_dirs[:3])\n",
    "print(\" Sets dirs:\", sets_dirs[:3])\n",
    "\n",
    "# Show sample image and transcription filenames\n",
    "if images_dirs:\n",
    "    sample_images = glob.glob(os.path.join(images_dirs[0], \"*\"))[:8]\n",
    "    print(\"\\nSample images:\")\n",
    "    for img in sample_images:\n",
    "        print(\"  \", img)\n",
    "\n",
    "if trans_dirs:\n",
    "    sample_trans = glob.glob(os.path.join(trans_dirs[0], \"*\"))[:8]\n",
    "    print(\"\\nSample transcription files:\")\n",
    "    for txt in sample_trans:\n",
    "        print(\"  \", txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf72efb-fa9b-4df1-ad25-b17e5781b50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSCRIPTION LOADER \n",
    "\n",
    "import os, re, glob\n",
    "from typing import Dict\n",
    "\n",
    "#  Utility functions \n",
    "\n",
    "def normalize_text(t: str) -> str:\n",
    "    \"\"\"Clean whitespace and remove extra spaces.\"\"\"\n",
    "    if not t:\n",
    "        return \"\"\n",
    "    t = t.strip()\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    return t\n",
    "\n",
    "def _read_text_file(path: str) -> str:\n",
    "    \"\"\"Read file safely with multiple encodings.\"\"\"\n",
    "    for enc in (\"utf-8\", \"latin1\"):\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=enc, errors=\"ignore\") as fh:\n",
    "                return fh.read()\n",
    "        except:\n",
    "            continue\n",
    "    return \"\"\n",
    "\n",
    "def normalize_key(k: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize the transcription key so it matches image filenames:\n",
    "       train2011-130_6 â†’ train2011-130_000006\n",
    "    \"\"\"\n",
    "    k = os.path.splitext(os.path.basename(k))[0]\n",
    "    if \"-\" in k and \"_\" in k:\n",
    "        prefix, num = k.split(\"_\", 1)\n",
    "        num = num.zfill(6)\n",
    "        return f\"{prefix}_{num}\"\n",
    "    return k\n",
    "\n",
    "# Main Loader \n",
    "\n",
    "def load_transcriptions(data_root: str) -> Dict[str, str]:\n",
    "    trans_map = {}\n",
    "\n",
    "    # Load ONLY per-image .txt files from Transcriptions folder\n",
    "    trans_paths = glob.glob(\n",
    "        os.path.join(data_root, \"**\", \"Transcriptions\", \"*.txt\"),\n",
    "        recursive=True\n",
    "    )\n",
    "\n",
    "    print(f\"Found {len(trans_paths)} transcription files in Transcriptions/ folder.\")\n",
    "\n",
    "    for p in sorted(trans_paths):\n",
    "        fname = os.path.basename(p).replace(\".txt\", \"\")\n",
    "        key = normalize_key(fname)\n",
    "\n",
    "        txt = _read_text_file(p).strip()\n",
    "        if not txt:\n",
    "            continue\n",
    "\n",
    "        # Remove quotes \"...\"\n",
    "        if txt.startswith('\"') and txt.endswith('\"'):\n",
    "            txt = txt[1:-1]\n",
    "\n",
    "        txt = normalize_text(txt)\n",
    "        if len(txt) < 2:\n",
    "            continue\n",
    "\n",
    "        trans_map[key] = txt\n",
    "\n",
    "    # We IGNORE mapping-style files (TrainLines.txt, TestLines.txt, etc.)\n",
    "    #    because they DO NOT contain actual text â†’ only filenames.\n",
    "\n",
    "    # Final cleanup of keys just to be safe\n",
    "    trans_map = {normalize_key(k): v for k, v in trans_map.items()}\n",
    "\n",
    "    print(f\"Loaded {len(trans_map)} valid transcriptions.\")\n",
    "    print(\"Example keys:\", list(trans_map.keys())[:8])\n",
    "    return trans_map\n",
    "\n",
    "# Assign global TRANS_MAP \n",
    "TRANS_MAP = load_transcriptions(DATA_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea49c28-8679-4968-8d97-31570745357c",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"eval2011-0_000001\"\n",
    "print(\"GT:\", TRANS_MAP.get(key))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93278a10-00ef-4a5a-84c4-c61308fccc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RWGD \n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "def RWGD_light(image, interval_at_h80=26, std_at_h80=1.7, target_h=80):\n",
    "    \"\"\"\n",
    "    Memory-optimized Random Warp Grid Distortion (RWGD) implementation.\n",
    "    Uses linear interpolation to save memory and prevent kernel crashes.\n",
    "\n",
    "    Args:\n",
    "        image: Grayscale handwriting image (uint8)\n",
    "        interval_at_h80: Grid interval at height=80 (default: 26)\n",
    "        std_at_h80: Standard deviation for displacement (default: 1.7)\n",
    "        target_h: Reference height (default: 80)\n",
    "    Returns:\n",
    "        Warped image \n",
    "    \"\"\"\n",
    "    h, w = image.shape\n",
    "    scale = h / target_h\n",
    "    interval = max(4, int(round(interval_at_h80 * scale)))\n",
    "    disp_std = float(std_at_h80 * scale)\n",
    "\n",
    "    gx = list(range(0, w, interval))\n",
    "    if gx[-1] != w - 1:\n",
    "        gx.append(w - 1)\n",
    "    gy = list(range(0, h, interval))\n",
    "    if gy[-1] != h - 1:\n",
    "        gy.append(h - 1)\n",
    "\n",
    "    src_pts = np.array([[x, y] for y in gy for x in gx], dtype=np.float32)\n",
    "    dst_pts = src_pts + np.random.normal(0, disp_std, src_pts.shape).astype(np.float32)\n",
    "\n",
    "    grid_x, grid_y = np.meshgrid(np.arange(w), np.arange(h))\n",
    "\n",
    "    # ðŸ”¹ Linear interpolation\n",
    "    disp_xi = griddata(\n",
    "        (src_pts[:, 0], src_pts[:, 1]),\n",
    "        dst_pts[:, 0] - src_pts[:, 0],\n",
    "        (grid_x, grid_y),\n",
    "        method=\"linear\",\n",
    "        fill_value=0.0\n",
    "    )\n",
    "    disp_yi = griddata(\n",
    "        (src_pts[:, 0], src_pts[:, 1]),\n",
    "        dst_pts[:, 1] - src_pts[:, 1],\n",
    "        (grid_x, grid_y),\n",
    "        method=\"linear\",\n",
    "        fill_value=0.0\n",
    "    )\n",
    "\n",
    "    map_x = (grid_x + disp_xi).astype(np.float32)\n",
    "    map_y = (grid_y + disp_yi).astype(np.float32)\n",
    "\n",
    "    warped = cv2.remap(\n",
    "        image, map_x, map_y, interpolation=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE\n",
    "    )\n",
    "    return warped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b9aa16-6dd0-4483-8cfe-fe605e0fa29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three-channel preprocessing \n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage.filters import threshold_local\n",
    "\n",
    "def howe_binarize(img: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Approximate Howe adaptive binarization.\n",
    "    \"\"\"\n",
    "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "    h = img.shape[0]\n",
    "\n",
    "    # Block size scaled with image height \n",
    "    block = int(min(max(15, h // 3), 55))  # keeps block size between 15â€“55\n",
    "    offset = 10  # as per-paper \n",
    "\n",
    "    try:\n",
    "        T = threshold_local(img, block_size=block, offset=offset)\n",
    "        out = (img > T).astype(np.uint8) * 255\n",
    "    except Exception as e:\n",
    "        # fallback: if threshold_local fails (rare on small images)\n",
    "        _, out = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    return out\n",
    "\n",
    "def otsu_binarize(img: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Standard Otsu global thresholding.\n",
    "    \"\"\"\n",
    "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "    _, th = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    return th\n",
    "\n",
    "def make_three_channel(img: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Combine grayscale, Otsu-binarized, and Howe-binarized versions\n",
    "    into a 3-channel (HÃ—WÃ—3) image for CNN input.\n",
    "    \"\"\"\n",
    "    if img.ndim == 3 and img.shape[2] == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "\n",
    "    g = img\n",
    "    b1 = otsu_binarize(img)\n",
    "    b2 = howe_binarize(img)\n",
    "    stacked = np.stack([g, b1, b2], axis=2)\n",
    "    return stacked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762d6446-6219-4662-b3d6-e642dc72045e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class HandWritingDataset\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class HandwritingDataset(Dataset):\n",
    "    def __init__(self, image_paths, trans_map, height=80, augment_type='rwgd',\n",
    "                 apply_pn=True, channel_mode='gray', dataset_type='line', seg_error_set=None):\n",
    "        \"\"\"\n",
    "        PyTorch dataset for handwriting line/word recognition.\n",
    "        \n",
    "        Args:\n",
    "            image_paths: list of image file paths\n",
    "            trans_map: dict basename -> ground truth transcription\n",
    "            height: target image height (default 80)\n",
    "            augment_type: 'none' | 'rwgd' | 'simard' | 'affine'\n",
    "            channel_mode: 'gray' or 'three'\n",
    "            dataset_type: 'line' or 'word'\n",
    "            seg_error_set: optional set of basenames to skip (bad segmentations)\n",
    "        \"\"\"\n",
    "        self.paths = image_paths\n",
    "        self.trans = trans_map\n",
    "        self.h = height\n",
    "        self.augment_type = augment_type\n",
    "        self.apply_pn = apply_pn\n",
    "        self.channel_mode = channel_mode\n",
    "        self.dataset_type = dataset_type\n",
    "        self.seg_error_set = seg_error_set if seg_error_set is not None else set()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.paths[idx]\n",
    "        b = os.path.basename(p)\n",
    "\n",
    "        # Skip bad segmentation \n",
    "        if b in self.seg_error_set:\n",
    "            img = np.ones((self.h, self.h), dtype=np.uint8) * 255\n",
    "            gt = \"\"\n",
    "            return img, gt, b\n",
    "\n",
    "        # Load image \n",
    "        img = cv2.imread(p, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None or img.size == 0:\n",
    "            raise RuntimeError(f\"Cannot read image: {p}\")\n",
    "\n",
    "        #  Profile Normalization\n",
    "        if self.apply_pn:\n",
    "            try:\n",
    "                img = profile_normalization(img, target_h=self.h)\n",
    "            except Exception as e:\n",
    "                print(f\"PN failed for {b}: {e}\")\n",
    "                img = cv2.resize(img, (max(1, int(img.shape[1])), self.h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # Resize height to match model input \n",
    "        img = cv2.resize(img, (max(1, int(img.shape[1])), self.h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        #Channel construction\n",
    "        if self.channel_mode == 'three':\n",
    "            img_ch = make_three_channel(img)  \n",
    "        else:\n",
    "            img_ch = img \n",
    "\n",
    "        # Augmentation \n",
    "        if self.augment_type == 'rwgd':\n",
    "            if self.channel_mode == 'three':\n",
    "                for c in range(3):\n",
    "                    img_ch[:, :, c] = RWGD_light(img_ch[:, :, c])\n",
    "            else:\n",
    "                img_ch = RWGD_light(img_ch)\n",
    "\n",
    "        elif self.augment_type == 'simard':\n",
    "            if self.channel_mode == 'three':\n",
    "                for c in range(3):\n",
    "                    img_ch[:, :, c] = simard_wrapper(img_ch[:, :, c])\n",
    "            else:\n",
    "                img_ch = simard_wrapper(img_ch)\n",
    "\n",
    "        elif self.augment_type == 'affine':\n",
    "            if self.channel_mode == 'three':\n",
    "                for c in range(3):\n",
    "                    img_ch[:, :, c] = affine_rotate_shear(img_ch[:, :, c])\n",
    "            else:\n",
    "                img_ch = affine_rotate_shear(img_ch)\n",
    "\n",
    "        #Robust ground-truth lookup fix (to stop 0 loss)\n",
    "        key= normalize_key(os.path.splitext(b)[0])\n",
    "        if key in self.trans:\n",
    "            gt = self.trans[key]\n",
    "        elif b in self.trans:\n",
    "            gt = self.trans[b]\n",
    "        else:\n",
    "            gt = \"\"\n",
    "\n",
    "        return img_ch, gt, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f319ca-4257-4e8d-8cff-0a78ec537da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate the Images folder\n",
    "import glob, os\n",
    "\n",
    "IMAGES_DIRS = glob.glob(os.path.join(DATA_ROOT, \"**\", \"Images\"), recursive=True)\n",
    "if IMAGES_DIRS:\n",
    "    IMAGES_DIR = IMAGES_DIRS[0]\n",
    "else:\n",
    "    candidates = []\n",
    "    for root, dirs, files in os.walk(DATA_ROOT):\n",
    "        imgs = [f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg', '.tif', '.tiff'))]\n",
    "        if len(imgs) > 20:\n",
    "            candidates.append(root)\n",
    "    if candidates:\n",
    "        IMAGES_DIR = candidates[0]\n",
    "    else:\n",
    "        raise RuntimeError(\" No image folder found under DATA_ROOT. Please check extraction structure.\")\n",
    "\n",
    "print(f\" Using IMAGES_DIR: {IMAGES_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26b7aa6-2367-464c-a40f-3dd1873fd229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def is_punctuation_only(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if the text contains ONLY punctuation or whitespace.\n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return True\n",
    "    return all(ch in string.punctuation for ch in text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4988ba1-3c89-45dc-9a62-4aabc17efea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET SPLITTING FUNCTION\n",
    "\n",
    "import os, glob, random\n",
    "\n",
    "def build_splits(\n",
    "    data_root, images_dir, trans_map, \n",
    "    test_filter_punct=True, \n",
    "    split=(0.8, 0.1, 0.1), \n",
    "    seed=42, \n",
    "    min_test_size=100\n",
    "):\n",
    "    random.seed(seed)\n",
    "\n",
    "    #  Collect images\n",
    "    images = sorted([\n",
    "        p for p in glob.glob(os.path.join(images_dir, \"*\"))\n",
    "        if p.lower().endswith(('.png', '.jpg', '.jpeg', '.tif', '.tiff'))\n",
    "    ])\n",
    "\n",
    "    if not images:\n",
    "        raise RuntimeError(f\"No image files found in {images_dir}\")\n",
    "\n",
    "    #  Deterministic shuffle \n",
    "    random.shuffle(images)\n",
    "    n = len(images)\n",
    "    n1 = int(n * split[0])\n",
    "    n2 = int(n * (split[0] + split[1]))\n",
    "\n",
    "    train = images[:n1]\n",
    "    val = images[n1:n2]\n",
    "    test = images[n2:]\n",
    "\n",
    "    #  Load segmentation error flags \n",
    "    seg_error_set = set()\n",
    "    possible_flag_files = glob.glob(os.path.join(data_root, \"**\", \"*seg*err*.*\"), recursive=True) + \\\n",
    "                          glob.glob(os.path.join(data_root, \"**\", \"*segmentation*.*\"), recursive=True)\n",
    "\n",
    "    for ff in possible_flag_files:\n",
    "        try:\n",
    "            with open(ff, encoding='utf-8', errors='ignore') as f:\n",
    "                for ln in f:\n",
    "                    ln = ln.strip()\n",
    "                    if ln:\n",
    "                        seg_error_set.add(os.path.basename(ln.split()[0]))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    #  Filter TEST set \n",
    "    if test_filter_punct:\n",
    "        test_filtered = []\n",
    "\n",
    "        for p in test:\n",
    "            b = os.path.basename(p)\n",
    "            key = normalize_key(os.path.splitext(b)[0])   # correct key\n",
    "            gt = trans_map.get(key, \"\")                  # correct GT from map\n",
    "\n",
    "            # Remove punctuation-only or too-short GT\n",
    "            if len(gt.strip()) < 2 or is_punctuation_only(gt):\n",
    "                continue\n",
    "\n",
    "            if b in seg_error_set:\n",
    "                continue\n",
    "\n",
    "            test_filtered.append(p)\n",
    "\n",
    "        # Guarantee non-empty test set\n",
    "        if len(test_filtered) < min_test_size:\n",
    "            print(f\"Too few test samples ({len(test_filtered)}). Restoring unfiltered test set.\")\n",
    "        else:\n",
    "            test = test_filtered\n",
    "\n",
    "    print(f\"Split complete: {len(train)} train, {len(val)} val, {len(test)} test | Seg errors: {len(seg_error_set)}\")\n",
    "    return train, val, test, seg_error_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90f6ccd-c2d6-4a43-8443-8c525707b42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list, val_list, test_list, seg_error_set = build_splits(\n",
    "    DATA_ROOT, IMAGES_DIR, TRANS_MAP\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629a9d59-488e-4143-8b44-a74776a78d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure charset builder exists\n",
    "def build_charset(trans_list):\n",
    "    \"\"\"Builds character vocabulary from transcriptions.\"\"\"\n",
    "    chars = set()\n",
    "    for t in trans_list:\n",
    "        if not isinstance(t, str):\n",
    "            continue\n",
    "        chars.update(list(t))\n",
    "    chars = sorted(list(chars))\n",
    "    itos = [''] + chars  # index-to-symbol (0 reserved for blank)\n",
    "    stoi = {c: i for i, c in enumerate(itos)}\n",
    "    return stoi, itos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d04fa67-5624-435c-8fde-415ef0e8f41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10 â€” CRNN model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, num_classes, in_channels=1, dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        chs = [in_channels, 64, 128, 256, 256, 512, 512]\n",
    "        layers = []\n",
    "        for i in range(1, len(chs)):\n",
    "            layers.append(nn.Conv2d(chs[i-1], chs[i], kernel_size=3, padding=1))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "\n",
    "            if i in [1, 2]:\n",
    "                layers.append(nn.MaxPool2d(2, 2))         # downscale both H and W\n",
    "            elif i in [4, 6]:\n",
    "                layers.append(nn.MaxPool2d((2, 1), (2, 1)))  # downscale only H\n",
    "\n",
    "            if i in [4, 5]:\n",
    "                layers.append(nn.BatchNorm2d(chs[i]))\n",
    "\n",
    "        self.cnn = nn.Sequential(*layers)\n",
    "        #adaptive average pooling to flatten height to 1\n",
    "        self.reduce_h = nn.AdaptiveAvgPool2d((1, None))\n",
    "\n",
    "        #BiLSTMs\n",
    "        self.rnn1 = nn.LSTM(512, 512, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.rnn2 = nn.LSTM(1024, 256, num_layers=1, bidirectional=True, batch_first=True)\n",
    "\n",
    "        #Output\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        self.logsoft = nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)                   # [B, C, H, W]\n",
    "        out = self.reduce_h(out)            # [B, C, 1, W]\n",
    "        out = out.squeeze(2)                # [B, C, W]\n",
    "        out = out.permute(0, 2, 1)          # [B, W, C]\n",
    "\n",
    "        out, _ = self.rnn1(out)\n",
    "        out = self.dropout(out)\n",
    "        out, _ = self.rnn2(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        out = self.logsoft(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# GPU setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Example test run \n",
    "stoi, itos = build_charset(list(TRANS_MAP.values()))\n",
    "num_classes = len(itos)\n",
    "\n",
    "model = CRNN(num_classes=num_classes, in_channels=3, dropout=0.5).to(device)\n",
    "\n",
    "# Test dummy input \n",
    "dummy = torch.randn(2, 3, 80, 400).to(device)\n",
    "with torch.no_grad():\n",
    "    out = model(dummy)\n",
    "\n",
    "print(\"Model output shape:\", out.shape)\n",
    "# [batch=2, width=??, classes=num_classes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dc0eaf-4d7a-4c9b-82fa-1db3cc48ce96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collate, encode/decode, lexicon-based scoring, evaluate with TTA\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import textdistance                        \n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Collation (pad/truncate widths)\n",
    "\n",
    "def collate_batch(batch, max_width=1600):\n",
    "    imgs, gts, keys = zip(*batch)\n",
    "    first = imgs[0]\n",
    "    # multi-channel case (H x W x C)\n",
    "    if first.ndim == 3:\n",
    "        C = first.shape[2]\n",
    "        H = first.shape[0]\n",
    "        W = min(max(img.shape[1] for img in imgs), max_width)\n",
    "        tensor = torch.zeros((len(imgs), C, H, W), dtype=torch.float32)  \n",
    "        for i, im in enumerate(imgs):\n",
    "            im_proc = im\n",
    "            if im_proc.ndim == 2:\n",
    "                im_proc = np.expand_dims(im_proc, 2)\n",
    "            h,w,c = im_proc.shape\n",
    "            w = min(w, W)\n",
    "            \n",
    "            im_norm = (255.0 - im_proc[:,:w,:]).astype(np.float32) / 255.0\n",
    "            tensor[i, :, :h, :w] = torch.tensor(im_norm.transpose(2,0,1))\n",
    "        return tensor, list(gts), list(keys)\n",
    "    else:\n",
    "        \n",
    "        H = first.shape[0]\n",
    "        W = min(max(img.shape[1] for img in imgs), max_width)\n",
    "        tensor = torch.zeros((len(imgs), 1, H, W), dtype=torch.float32)\n",
    "        for i, im in enumerate(imgs):\n",
    "            w = min(W, im.shape[1])\n",
    "            im_norm = (255.0 - im[:, :w]).astype(np.float32) / 255.0\n",
    "            tensor[i,0,:im_norm.shape[0],:im_norm.shape[1]] = torch.tensor(im_norm)\n",
    "        return tensor, list(gts), list(keys)\n",
    "\n",
    "\n",
    "# Text encoding/decoding helpers\n",
    "\n",
    "def encode_text(s, stoi):\n",
    "    s = normalize_text(s)\n",
    "    return [stoi.get(ch, 0) for ch in s]\n",
    "\n",
    "def greedy_decode(log_probs): \n",
    "    idx = torch.argmax(log_probs, dim=2)\n",
    "    result = []\n",
    "    for row in idx:\n",
    "        prev = None\n",
    "        out = []\n",
    "        for a in row.cpu().numpy().tolist():\n",
    "            if a != prev and a != 0:\n",
    "                out.append(a)\n",
    "            prev = a\n",
    "        result.append(out)\n",
    "    return result\n",
    "\n",
    "def indices_to_text(indices, itos):\n",
    "    return ''.join([itos[i] for i in indices])\n",
    "\n",
    "\n",
    "# Distance / metrics (use textdistance)\n",
    "\n",
    "def cer(ref, hyp):\n",
    "    # character-level Levenshtein\n",
    "    return textdistance.levenshtein.distance(ref, hyp) / max(1, len(ref))\n",
    "\n",
    "def wer(ref, hyp):\n",
    "    r = ref.split(); h = hyp.split()\n",
    "    if len(r) == 0:\n",
    "        return 0.0 if len(h) == 0 else 1.0\n",
    "\n",
    "    return textdistance.levenshtein.distance(r, h) / max(1, len(r))\n",
    "\n",
    "\n",
    "# CTC loss and lexicon helpers\n",
    "\n",
    "ctc_loss_fn = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "\n",
    "def prune_lexicon_by_edit(greedy_txt, lexicon, max_dist=4):\n",
    "    candidates = []\n",
    "    for w in lexicon:\n",
    "        if abs(len(w) - len(greedy_txt)) > max_dist:\n",
    "            continue\n",
    "        d = textdistance.levenshtein.distance(greedy_txt, w)\n",
    "        if d <= max_dist:\n",
    "            candidates.append((d,w))\n",
    "    candidates.sort()\n",
    "    return [w for _,w in candidates]\n",
    "\n",
    "def best_lexicon_ctc_choice(log_probs_tensor, lexicon, stoi, max_prune_dist=4):\n",
    "    \n",
    "    if log_probs_tensor.dim() == 3:\n",
    "        logp = log_probs_tensor[0]\n",
    "    else:\n",
    "        logp = log_probs_tensor\n",
    "    # greedy text\n",
    "    greedy_idx = torch.argmax(logp, dim=1).cpu().numpy().tolist()\n",
    "    prev = None\n",
    "    greedy_inds = []\n",
    "    for a in greedy_idx:\n",
    "        if a != prev and a != 0:\n",
    "            greedy_inds.append(a)\n",
    "        prev = a\n",
    "    \n",
    "    greedy_txt = indices_to_text(greedy_inds, itos)\n",
    "    candidates = prune_lexicon_by_edit(greedy_txt, lexicon, max_prune_dist)\n",
    "    if not candidates:\n",
    "        return greedy_txt, None\n",
    "    best_w, best_loss = None, float('inf')\n",
    "    T, C = logp.shape\n",
    "    input_len = torch.tensor([T], dtype=torch.long)\n",
    "    for w in candidates:\n",
    "        lab = torch.tensor([stoi.get(ch,0) for ch in w], dtype=torch.long)\n",
    "        # logp: T x C -> needs shape (T, C) and ctc expects (T, N, C) or (N, T, C) depending; we adapt:\n",
    "        # Our ctc usage: log_probs.unsqueeze(1) (T x C -> 1 x T x C) and labs shaped accordingly below\n",
    "        try:\n",
    "            loss = ctc_loss_fn(logp.unsqueeze(1), lab.unsqueeze(0), input_len, torch.tensor([len(lab)], dtype=torch.long))\n",
    "        except Exception:\n",
    "            # fallback if shapes mismatch; skip this candidate\n",
    "            continue\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            best_w = w\n",
    "    return best_w, best_loss\n",
    "\n",
    "\n",
    "# Evaluation with optional TTA and lexicon scoring\n",
    "\n",
    "def evaluate_model(model, dataloader, stoi, itos, device, dataset_type='line',\n",
    "                   lexicon=None, test_time_N=1, use_rwgd_on_test=True):\n",
    "    model.eval()\n",
    "    total_cer = 0.0\n",
    "    total_wer = 0.0\n",
    "    n = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            imgs, gts, keys = batch\n",
    "            B = imgs.size(0)\n",
    "            for i in range(B):\n",
    "                \n",
    "                if imgs.dim() == 4:\n",
    "                    \n",
    "                    if imgs.size(1) > 1:\n",
    "                        im_np = (255.0 - imgs[i].cpu().numpy()*255.0).transpose(1,2,0)\n",
    "                        if isinstance(im_np, np.ndarray) and im_np.ndim==3 and im_np.shape[2]==1:\n",
    "                            im_np = im_np[:,:,0]\n",
    "                    else:\n",
    "                        im_np = (255.0 - imgs[i,0].cpu().numpy()*255.0)\n",
    "                else:\n",
    "                    im_np = imgs[i].cpu().numpy()\n",
    "\n",
    "                # Run TTA variants\n",
    "                variants = []\n",
    "                for k in range(test_time_N):\n",
    "                    if use_rwgd_on_test and getattr(dataloader.dataset, \"augment_type\", \"\") == 'rwgd':\n",
    "                        # apply memory-light RWGD on numpy image (single-channel or per-channel)\n",
    "                        if isinstance(im_np, np.ndarray) and im_np.ndim==2:\n",
    "                            v = RWGD_light(im_np.astype(np.uint8))\n",
    "                        elif isinstance(im_np, np.ndarray) and im_np.ndim==3:\n",
    "                            v = im_np.copy()\n",
    "                            for ch in range(v.shape[2]):\n",
    "                                v[:,:,ch] = RWGD_light(v[:,:,ch].astype(np.uint8))\n",
    "                        else:\n",
    "                            v = im_np\n",
    "                    else:\n",
    "                        v = im_np\n",
    "\n",
    "                    # prepare tensor (C x H x W) normalized as model expects\n",
    "                    if isinstance(v, np.ndarray) and v.ndim==2:\n",
    "                        v_t = torch.tensor((255.0 - v).astype(np.float32)/255.0)[None,None].to(device)\n",
    "                    else:\n",
    "                        # H x W x C -> C x H x W\n",
    "                        v_t = torch.tensor((255.0 - v).astype(np.float32)/255.0).transpose(2,0,1)[None].to(device)\n",
    "\n",
    "                    out = model(v_t)  # 1 x T x C\n",
    "                    variants.append(out.cpu())\n",
    "\n",
    "                # Decide final prediction\n",
    "                final_text = \"\"\n",
    "                if lexicon is not None and dataset_type == 'word':\n",
    "                    best_overall = None\n",
    "                    best_loss = float('inf')\n",
    "                    for v_out in variants:\n",
    "                        cand, loss = best_lexicon_ctc_choice(v_out, lexicon, stoi, max_prune_dist=4)\n",
    "                        if loss is not None and loss < best_loss:\n",
    "                            best_loss = loss\n",
    "                            best_overall = cand\n",
    "                    if best_overall is None:\n",
    "                        greedy_preds = [indices_to_text(greedy_decode(v)[0], itos) for v in variants]\n",
    "                        final_text = Counter(greedy_preds).most_common(1)[0][0]\n",
    "                    else:\n",
    "                        final_text = best_overall\n",
    "                else:\n",
    "                    greedy_preds = [indices_to_text(greedy_decode(v)[0], itos) for v in variants]\n",
    "                    final_text = Counter(greedy_preds).most_common(1)[0][0]\n",
    "\n",
    "                ref = normalize_text(gts[i])\n",
    "                hyp = final_text\n",
    "                total_cer += cer(ref, hyp)\n",
    "                total_wer += wer(ref, hyp)\n",
    "                n += 1\n",
    "\n",
    "    return total_cer / max(1, n), total_wer / max(1, n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3592bd-077b-4aca-9820-04ba8b3bb00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "non_empty = 0\n",
    "empty = 0\n",
    "\n",
    "for p in train_list[:200]:\n",
    "    b = os.path.basename(p)\n",
    "    key = os.path.splitext(b)[0]\n",
    "    gt = TRANS_MAP.get(key, \"\")\n",
    "    if gt.strip():\n",
    "        non_empty += 1\n",
    "    else:\n",
    "        empty += 1\n",
    "\n",
    "print(f\" Non-empty transcriptions: {non_empty}\")\n",
    "print(f\"Empty transcriptions: {empty}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edec066-53f0-45df-b346-94021843ae38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Profile Normalization \n",
    "import numpy as np, cv2, math, gc\n",
    "\n",
    "def compute_horizontal_profile_std(img: np.ndarray) -> float:\n",
    "    \"\"\"Compute Ïƒ of the horizontal projection profile (line height estimate).\"\"\"\n",
    "    inv = 255 - img\n",
    "    profile = inv.sum(axis=1)\n",
    "    total = profile.sum() + 1e-8\n",
    "    if total < 1e-8:\n",
    "        return 1.0\n",
    "    idx = np.arange(len(profile), dtype=np.float32)\n",
    "    mean = (profile * idx).sum() / total\n",
    "    var = (profile * (idx - mean) ** 2).sum() / total\n",
    "    return float(math.sqrt(var + 1e-8))\n",
    "\n",
    "def profile_normalization(img: np.ndarray,\n",
    "                          target_h: int = 80,\n",
    "                          r: float = 1.75,\n",
    "                          ref_baseline: float = 16.0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Lightweight profile normalization:\n",
    "      â€¢ Scales image vertically by variance of its horizontal projection\n",
    "      â€¢ Centers text vertically on a fixed-height canvas\n",
    "      â€¢ Uses minimal temporary arrays for memory safety\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sigma = compute_horizontal_profile_std(img)\n",
    "        sigma = max(sigma, 1e-3)\n",
    "        s = ref_baseline / (sigma * r)\n",
    "\n",
    "        new_h = max(8, int(round(img.shape[0] * s)))\n",
    "        new_w = max(1, int(round(img.shape[1] * s)))\n",
    "\n",
    "        resized = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        inv = 255 - resized\n",
    "        total = inv.sum() + 1e-8\n",
    "        rows = np.arange(resized.shape[0], dtype=np.float32)\n",
    "        r_mean = float((inv.sum(axis=1) * rows).sum() / total)\n",
    "\n",
    "        canvas = np.full((target_h, resized.shape[1]), 255, dtype=np.uint8)\n",
    "        target_r = (target_h - 1) / 2.0\n",
    "        shift = int(round(target_r - r_mean))\n",
    "\n",
    "        y0 = max(0, shift)\n",
    "        y1 = min(target_h, shift + resized.shape[0])\n",
    "        src0 = max(0, -shift)\n",
    "        src1 = src0 + (y1 - y0)\n",
    "\n",
    "        if y0 < y1 and src0 < src1:\n",
    "            canvas[y0:y1, :] = resized[src0:src1, :]\n",
    "\n",
    "        # explicit cleanup\n",
    "        del resized, inv, rows\n",
    "        gc.collect()\n",
    "        return canvas\n",
    "\n",
    "    except Exception as e:\n",
    "        # fallback to simple resize\n",
    "        print(f\"PN fallback: {e}\")\n",
    "        return cv2.resize(img, (max(1, int(img.shape[1])), target_h),\n",
    "                          interpolation=cv2.INTER_AREA)\n",
    "\n",
    "print(\"profile_normalization ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f70a6c-1c22-4149-a05c-15fb4cfbae46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TINY TRAINING PIPELINE (FAST â€” 40 samples)\n",
    "\n",
    "\n",
    "import os, cv2, torch, glob, random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1. SMALL DATASET CREATION\n",
    "\n",
    "\n",
    "# We assume TRANS_MAP, train_list, val_list, test_list already exist.\n",
    "# Just pick small subsets.\n",
    "\n",
    "small_train = train_list[:40]     # 40 training images\n",
    "small_val   = val_list[:5]        # 5 validation images\n",
    "small_test  = test_list[:5]       # 5 test images\n",
    "\n",
    "print(\"Using tiny dataset:\")\n",
    "print(\"Train:\", len(small_train))\n",
    "print(\"Val:\", len(small_val))\n",
    "print(\"Test:\", len(small_test))\n",
    "\n",
    "\n",
    "\n",
    "# 2. Minimal Normalization\n",
    "\n",
    "\n",
    "def normalize_text(t):\n",
    "    return t.strip() if isinstance(t, str) else \"\"\n",
    "\n",
    "\n",
    "# 3. Tiny Dataset Loader (simple, no caching needed)\n",
    "\n",
    "\n",
    "class TinyDataset(Dataset):\n",
    "    def __init__(self, paths, trans_map, height=80):\n",
    "        self.paths = paths\n",
    "        self.trans = trans_map\n",
    "        self.h = height\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.paths[idx]\n",
    "        key = os.path.splitext(os.path.basename(p))[0]\n",
    "\n",
    "        # Load grayscale\n",
    "        img = cv2.imread(p, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            raise RuntimeError(f\"Cannot load image: {p}\")\n",
    "\n",
    "        # simple height resize\n",
    "        scale = self.h / img.shape[0]\n",
    "        new_w = int(img.shape[1] * scale)\n",
    "        img = cv2.resize(img, (new_w, self.h))\n",
    "\n",
    "        # Normalize 0â€“1, invert (text = white)\n",
    "        img = (255 - img).astype(np.float32) / 255.0\n",
    "        img = img[None, :, :]  # (1, H, W)\n",
    "\n",
    "        gt = self.trans.get(key, \"\")\n",
    "        return img, gt, key\n",
    "\n",
    "\n",
    "\n",
    "# 4. Collate Function\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    imgs, gts, keys = zip(*batch)\n",
    "    max_w = max(im.shape[2] for im in imgs)\n",
    "    padded = torch.zeros((len(imgs), 1, imgs[0].shape[1], max_w))\n",
    "\n",
    "    for i, im in enumerate(imgs):\n",
    "        w = im.shape[2]\n",
    "        padded[i, :, :, :w] = torch.tensor(im)\n",
    "\n",
    "    return padded, list(gts), list(keys)\n",
    "\n",
    "\n",
    "\n",
    "# 5. Build Charset (stoi, itos)\n",
    "\n",
    "\n",
    "def build_charset(texts):\n",
    "    chars = set()\n",
    "    for t in texts:\n",
    "        chars.update(list(t))\n",
    "    chars = sorted(list(chars))\n",
    "    itos = [''] + chars\n",
    "    stoi = {c: i for i, c in enumerate(itos)}\n",
    "    return stoi, itos\n",
    "\n",
    "train_texts = [normalize_text(TRANS_MAP.get(os.path.splitext(os.path.basename(p))[0], \"\")) \n",
    "               for p in small_train]\n",
    "stoi, itos = build_charset(train_texts)\n",
    "num_classes = len(itos)\n",
    "\n",
    "\n",
    "\n",
    "# 6. CRNN Model (Simplified)\n",
    "\n",
    "\n",
    "class SimpleCRNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU()\n",
    "        )\n",
    "        self.rnn = nn.LSTM(128*40, 256, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        self.logsoft = nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)                 # [B,128,40,W/2]\n",
    "        B,C,H,W = out.shape\n",
    "        out = out.permute(0,3,1,2)        # [B,W/2,C,H]\n",
    "        out = out.reshape(B, W, C*H)      # [B,T,features]\n",
    "        out,_ = self.rnn(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "# 7. Encoding + Greedy Decoding\n",
    "\n",
    "\n",
    "def encode_text(t, stoi):\n",
    "    return [stoi.get(c, 0) for c in t]\n",
    "\n",
    "def greedy_decode(logits):\n",
    "    idx = torch.argmax(logits, dim=2)[0].cpu().tolist()\n",
    "    prev = None\n",
    "    seq = []\n",
    "    for i in idx:\n",
    "        if i != prev and i != 0:\n",
    "            seq.append(i)\n",
    "        prev = i\n",
    "    return ''.join(itos[i] for i in seq)\n",
    "\n",
    "\n",
    "\n",
    "# 8. Train for 2 epochs (FAST)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SimpleCRNN(num_classes).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "ctc = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "\n",
    "train_ds = TinyDataset(small_train, TRANS_MAP)\n",
    "val_ds   = TinyDataset(small_val, TRANS_MAP)\n",
    "test_ds  = TinyDataset(small_test, TRANS_MAP)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_batch)\n",
    "val_loader   = DataLoader(val_ds, batch_size=1, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader  = DataLoader(test_ds, batch_size=1, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "for epoch in range(2):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for imgs, gts, _ in train_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        logits = model(imgs)\n",
    "        B,T,C = logits.shape\n",
    "\n",
    "        targets = []\n",
    "        lengths = []\n",
    "        for gt in gts:\n",
    "            enc = encode_text(gt, stoi)\n",
    "            if enc:\n",
    "                targets.extend(enc)\n",
    "                lengths.append(len(enc))\n",
    "\n",
    "        if not lengths:\n",
    "            continue\n",
    "\n",
    "        targets = torch.tensor(targets, dtype=torch.long).to(device)\n",
    "        input_len = torch.tensor([T]*len(lengths), dtype=torch.long)\n",
    "        target_len = torch.tensor(lengths, dtype=torch.long)\n",
    "\n",
    "        loss = ctc(logits.permute(1,0,2), targets, input_len, target_len)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Loss: {total_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# 9. Evaluate on tiny test set\n",
    "\n",
    "\n",
    "def CER(a, b):\n",
    "    import textdistance\n",
    "    return textdistance.levenshtein.distance(a, b) / max(1, len(a))\n",
    "\n",
    "def WER(a, b):\n",
    "    import textdistance\n",
    "    return textdistance.levenshtein.distance(a.split(), b.split()) / max(1, len(a.split()))\n",
    "\n",
    "cer_total, wer_total, count = 0,0,0\n",
    "\n",
    "print(\"\\nPredictions on tiny test set:\\n\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for imgs, gts, keys in test_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        out = model(imgs)\n",
    "        pred = greedy_decode(out)\n",
    "\n",
    "        gt = gts[0]\n",
    "\n",
    "        print(\"Image:\", keys[0])\n",
    "        print(\"GT:   \", gt)\n",
    "        print(\"Pred: \", pred)\n",
    "        print(\"-\"*40)\n",
    "\n",
    "        cer_total += CER(gt, pred)\n",
    "        wer_total += WER(gt, pred)\n",
    "        count += 1\n",
    "\n",
    "print(\"\\nFINAL TINY RESULTS:\")\n",
    "print(\"CER:\", cer_total / count)\n",
    "print(\"WER:\", wer_total / count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c32afe-1fe7-43ab-b3ca-19ad94d04904",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  FINAL TRAINING SCRIPT \n",
    "\n",
    "import os, re, glob, gc, math, time\n",
    "from typing import Dict\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import amp\n",
    "\n",
    "\n",
    "#  PROFILE NORMALIZATION \n",
    "\n",
    "def compute_horizontal_profile_std(img: np.ndarray) -> float:\n",
    "    inv = 255 - img\n",
    "    profile = inv.sum(axis=1)\n",
    "    total = profile.sum() + 1e-8\n",
    "    if total < 1e-8:\n",
    "        return 1.0\n",
    "    idx = np.arange(len(profile), dtype=np.float32)\n",
    "    mean = (profile * idx).sum() / total\n",
    "    var = (profile * (idx - mean) ** 2).sum() / total\n",
    "    return float(math.sqrt(var + 1e-8))\n",
    "\n",
    "def profile_normalization(img: np.ndarray, target_h=80, r=1.75, ref_baseline=16.0):\n",
    "    try:\n",
    "        sigma = compute_horizontal_profile_std(img)\n",
    "        sigma = max(sigma, 1e-3)\n",
    "        s = ref_baseline / (sigma * r)\n",
    "        new_h = max(8, int(round(img.shape[0] * s)))\n",
    "        new_w = max(1, int(round(img.shape[1] * s)))\n",
    "        resized = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        inv = 255 - resized\n",
    "        total = inv.sum() + 1e-8\n",
    "        rows = np.arange(resized.shape[0], dtype=np.float32)\n",
    "        r_mean = float((inv.sum(axis=1) * rows).sum() / total)\n",
    "\n",
    "        canvas = np.full((target_h, resized.shape[1]), 255, dtype=np.uint8)\n",
    "        target_r = (target_h - 1) / 2.0\n",
    "        shift = int(round(target_r - r_mean))\n",
    "        y0, y1 = max(0, shift), min(target_h, shift + resized.shape[0])\n",
    "        src0, src1 = max(0, -shift), max(0, -shift) + (y1 - y0)\n",
    "\n",
    "        if y0 < y1 and src0 < src1:\n",
    "            canvas[y0:y1, :] = resized[src0:src1, :]\n",
    "\n",
    "        del resized, inv, rows\n",
    "        gc.collect()\n",
    "        return canvas\n",
    "    except Exception as e:\n",
    "        print(f\"PN fallback: {e}\")\n",
    "        return cv2.resize(img, (max(1, int(img.shape[1])), target_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "print(\"Profile Normalization ready.\")\n",
    "\n",
    "\n",
    "#  TRANSCRIPTION LOADER  \n",
    "\n",
    "def normalize_text(t):\n",
    "    if t is None:\n",
    "        return \"\"\n",
    "    t = re.sub(r\"\\s+\", \" \", t.strip())\n",
    "    return t\n",
    "\n",
    "def load_transcriptions(data_root: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Load only the per-image transcription files from Transcriptions/ folder.\n",
    "    This avoids reading Set files (TrainLines/TestLines) that contain filenames only.\n",
    "    \"\"\"\n",
    "    trans_map = {}\n",
    "    trans_files = glob.glob(os.path.join(data_root, \"**\", \"Transcriptions\", \"*.txt\"), recursive=True)\n",
    "    if not trans_files:\n",
    "        # fallback: sometimes transcriptions are saved in a top-level Transcriptions folder name variant\n",
    "        trans_files = glob.glob(os.path.join(data_root, \"**\", \"*Transcriptions*\", \"*.txt\"), recursive=True)\n",
    "\n",
    "    for p in sorted(trans_files):\n",
    "        try:\n",
    "            txt = open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\").read().strip()\n",
    "        except Exception:\n",
    "            txt = \"\"\n",
    "        base = os.path.splitext(os.path.basename(p))[0]\n",
    "        # remove surrounding quotes if any\n",
    "        if txt.startswith('\"') and txt.endswith('\"'):\n",
    "            txt = txt[1:-1]\n",
    "        trans_map[base] = normalize_text(txt)\n",
    "    print(f\"Loaded {len(trans_map)} valid transcription files from Transcriptions/.\")\n",
    "    return trans_map\n",
    "\n",
    "#  Ensure DATA_ROOT \n",
    "try:\n",
    "    DATA_ROOT\n",
    "except NameError:\n",
    "    DATA_ROOT = r\"E:\\PYTHON\\CV_Project2\\dataset\\extracted\"\n",
    "    print(\"DATA_ROOT not found â€” using default:\", DATA_ROOT)\n",
    "\n",
    "TRANS_MAP = load_transcriptions(DATA_ROOT)\n",
    "\n",
    "non_empty = sum(1 for v in TRANS_MAP.values() if v.strip())\n",
    "empty = sum(1 for v in TRANS_MAP.values() if not v.strip())\n",
    "print(f\"TRANS_MAP: {len(TRANS_MAP)} entries | Non-empty: {non_empty} | Empty: {empty}\")\n",
    "\n",
    "\n",
    "#  DATASET & CACHE CLASS \n",
    "\n",
    "class HandwritingDataset(Dataset):\n",
    "    def __init__(self, image_paths, trans_map, height=80, apply_pn=True, cache_dir=\"cache_png\", seg_error_set=None):\n",
    "        self.paths = image_paths\n",
    "        self.trans = trans_map\n",
    "        self.h = height\n",
    "        self.apply_pn = apply_pn\n",
    "        self.cache_dir = cache_dir\n",
    "        self.seg_error_set = seg_error_set if seg_error_set else set()\n",
    "        if self.cache_dir:\n",
    "            os.makedirs(self.cache_dir, exist_ok=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.paths[idx]\n",
    "        key = os.path.splitext(os.path.basename(p))[0]\n",
    "\n",
    "        if key in self.seg_error_set:\n",
    "            return np.ones((self.h, self.h), np.uint8) * 255, \"\", key\n",
    "\n",
    "        cache_path = os.path.join(self.cache_dir, key + \".npy\")\n",
    "        if os.path.exists(cache_path):\n",
    "            img = np.load(cache_path)\n",
    "        else:\n",
    "            img = cv2.imread(p, cv2.IMREAD_GRAYSCALE)\n",
    "            if img is None:\n",
    "                raise RuntimeError(f\"Cannot read image: {p}\")\n",
    "            if self.apply_pn:\n",
    "                img = profile_normalization(img, target_h=self.h)\n",
    "            img = cv2.resize(img, (max(1, int(img.shape[1])), self.h), interpolation=cv2.INTER_AREA)\n",
    "            np.save(cache_path, img)\n",
    "        gt = self.trans.get(key, \"\")\n",
    "        return img, gt, key\n",
    "\n",
    "print(\"HandwritingDataset ready (cached).\")\n",
    "\n",
    "\n",
    "#  COLLATE & HELPERS  \n",
    "\n",
    "def collate_batch(batch, max_width=1600):\n",
    "    imgs, gts, keys = zip(*batch)\n",
    "    first = imgs[0]\n",
    "    # multi-channel not used (we're single-channel)\n",
    "    H = first.shape[0]\n",
    "    W = min(max(img.shape[1] for img in imgs), max_width)\n",
    "    tensor = torch.zeros((len(imgs), 1, H, W), dtype=torch.float32)\n",
    "    for i, im in enumerate(imgs):\n",
    "        w = min(W, im.shape[1])\n",
    "        im_norm = (255.0 - im[:, :w]).astype(np.float32) / 255.0\n",
    "        tensor[i,0,:im_norm.shape[0],:im_norm.shape[1]] = torch.tensor(im_norm)\n",
    "    return tensor, list(gts), list(keys)\n",
    "\n",
    "def encode_text(s, stoi):\n",
    "    s = normalize_text(s)\n",
    "    return [stoi.get(ch, 0) for ch in s]\n",
    "\n",
    "def greedy_decode(log_probs):  # log_probs: B x T x C\n",
    "    idx = torch.argmax(log_probs, dim=2)\n",
    "    result = []\n",
    "    for row in idx:\n",
    "        prev = None\n",
    "        out = []\n",
    "        for a in row.cpu().numpy().tolist():\n",
    "            if a != prev and a != 0:\n",
    "                out.append(a)\n",
    "            prev = a\n",
    "        result.append(out)\n",
    "    return result\n",
    "\n",
    "def indices_to_text(indices, itos):\n",
    "    return ''.join([itos[i] for i in indices])\n",
    "\n",
    "# Levenshtein-based metrics \n",
    "import textdistance\n",
    "def cer(ref, hyp):\n",
    "    return textdistance.levenshtein.distance(ref, hyp) / max(1, len(ref))\n",
    "\n",
    "def wer(ref, hyp):\n",
    "    r = ref.split(); h = hyp.split()\n",
    "    if len(r) == 0:\n",
    "        return 0.0 if len(h) == 0 else 1.0\n",
    "    return textdistance.levenshtein.distance(r, h) / max(1, len(r))\n",
    "\n",
    "# simple charset builde\n",
    "def build_charset(trans_list):\n",
    "    chars = set()\n",
    "    for t in trans_list:\n",
    "        if not isinstance(t, str):\n",
    "            continue\n",
    "        chars.update(list(t))\n",
    "    chars = sorted(list(chars))\n",
    "    itos = [''] + chars\n",
    "    stoi = {c:i for i,c in enumerate(itos)}\n",
    "    return stoi, itos\n",
    "\n",
    "\n",
    "# CRNN model \n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, num_classes, in_channels=1, dropout=0.5):\n",
    "        super().__init__()\n",
    "        chs = [in_channels, 64, 128, 256, 256, 512, 512]\n",
    "        layers = []\n",
    "        for i in range(1, len(chs)):\n",
    "            layers.append(nn.Conv2d(chs[i-1], chs[i], kernel_size=3, padding=1))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            if i in [1,2]:\n",
    "                layers.append(nn.MaxPool2d(2,2))\n",
    "            elif i in [4,6]:\n",
    "                layers.append(nn.MaxPool2d((2,1),(2,1)))\n",
    "            if i in [4,5]:\n",
    "                layers.append(nn.BatchNorm2d(chs[i]))\n",
    "        self.cnn = nn.Sequential(*layers)\n",
    "        self.reduce_h = nn.AdaptiveAvgPool2d((1, None))\n",
    "        self.rnn1 = nn.LSTM(512, 512, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.rnn2 = nn.LSTM(1024, 256, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        self.logsoft = nn.LogSoftmax(dim=2)\n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)                   # [B,C,H,W]\n",
    "        out = self.reduce_h(out)            # [B,C,1,W]\n",
    "        out = out.squeeze(2)                # [B,C,W]\n",
    "        out = out.permute(0,2,1)            # [B,W,C]\n",
    "        out, _ = self.rnn1(out)\n",
    "        out = self.dropout(out)\n",
    "        out, _ = self.rnn2(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        out = self.logsoft(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "#  CTC loss & eval helpers \n",
    "\n",
    "ctc_loss_fn = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "\n",
    "def evaluate_model(model, dataloader, stoi, itos, device, dataset_type='line',\n",
    "                   lexicon=None, test_time_N=1, use_rwgd_on_test=False):\n",
    "    model.eval()\n",
    "    total_cer = 0.0\n",
    "    total_wer = 0.0\n",
    "    n = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            imgs, gts, keys = batch\n",
    "            B = imgs.size(0)\n",
    "            for i in range(B):\n",
    "                # prepare single input\n",
    "                if imgs.size(1) > 1:\n",
    "                    im_np = (255.0 - imgs[i].cpu().numpy()*255.0).transpose(1,2,0)\n",
    "                    if isinstance(im_np, np.ndarray) and im_np.ndim==3 and im_np.shape[2]==1:\n",
    "                        im_np = im_np[:,:,0]\n",
    "                else:\n",
    "                    im_np = (255.0 - imgs[i,0].cpu().numpy()*255.0)\n",
    "                # prepare tensor\n",
    "                v_t = torch.tensor((im_np).astype(np.float32)/255.0)[None,None].to(device)\n",
    "                out = model(v_t)\n",
    "                # decode\n",
    "                greedy_inds = greedy_decode(out.cpu())[0]\n",
    "                hyp = indices_to_text(greedy_inds, itos)\n",
    "                ref = normalize_text(gts[i])\n",
    "                total_cer += cer(ref, hyp)\n",
    "                total_wer += wer(ref, hyp)\n",
    "                n += 1\n",
    "    return total_cer / max(1, n), total_wer / max(1, n)\n",
    "\n",
    "\n",
    "# === TRAIN/EVAL LOOPS \n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, stoi, device, scaler, log_interval=200):\n",
    "    model.train()\n",
    "    total_loss, batches = 0.0, 0\n",
    "    for batch_idx, batch in enumerate(loader):\n",
    "        imgs, gts, _ = batch\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        with amp.autocast(device_type='cuda', enabled=(device.type=='cuda')):\n",
    "            out = model(imgs)             # B x T x C\n",
    "            B, T, C = out.shape\n",
    "            targets, target_lens, valid_idx = [], [], []\n",
    "            for i, gt in enumerate(gts):\n",
    "                lab = encode_text(gt, stoi)\n",
    "                if len(lab) > 0:\n",
    "                    valid_idx.append(i)\n",
    "                    targets.extend(lab)\n",
    "                    target_lens.append(len(lab))\n",
    "            if not valid_idx:\n",
    "                continue\n",
    "            out_valid = out[valid_idx]             # NxTxc\n",
    "            targets = torch.tensor(targets, dtype=torch.long, device=device)\n",
    "            input_lens = torch.full((len(valid_idx),), T, dtype=torch.long, device=device)\n",
    "            target_lens = torch.tensor(target_lens, dtype=torch.long, device=device)\n",
    "            loss = ctc_loss_fn(out_valid.permute(1,0,2), targets, input_lens, target_lens)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "        batches += 1\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            print(f\"  Batch {batch_idx+1}: loss={loss.item():.4f}\")\n",
    "        if (batch_idx + 1) % 50 == 0:\n",
    "            if device.type == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    return total_loss / max(1, batches)\n",
    "\n",
    "\n",
    "# RUN EXPERIMENT\n",
    "def run_experiment_timed(augment_type='none', epochs=10, batch_size=8,\n",
    "                         use_pn=True, channel_mode='gray', fast_mode=False,\n",
    "                         patience=3, min_epochs=4):\n",
    "    print(f\"\\n=== Experiment: augment={augment_type}, channel={channel_mode} ===\")\n",
    "    global train_list, val_list, test_list, seg_error_set, DATA_TYPE\n",
    "    if \"DATA_TYPE\" not in globals():\n",
    "        DATA_TYPE = \"word\" if any(\"word\" in d.lower() for d in os.listdir(DATA_ROOT)) else \"line\"\n",
    "        print(f\" Auto-detected DATA_TYPE = '{DATA_TYPE}'\")\n",
    "    if fast_mode:\n",
    "        train_subset, val_subset, test_subset = train_list[:300], val_list[:100], test_list[:50]\n",
    "        print(\" Fast-mode enabled (small subsets).\")\n",
    "    else:\n",
    "        train_subset, val_subset, test_subset = train_list, val_list, test_list\n",
    "        print(f\" Using full dataset: {len(train_subset)} train / {len(val_subset)} val / {len(test_subset)} test\")\n",
    "\n",
    "    train_ds = HandwritingDataset(train_subset, TRANS_MAP, height=80, apply_pn=use_pn, cache_dir=\"cache_png\", seg_error_set=seg_error_set)\n",
    "    val_ds = HandwritingDataset(val_subset, TRANS_MAP, height=80, apply_pn=use_pn, cache_dir=\"cache_png\")\n",
    "    test_ds = HandwritingDataset(test_subset, TRANS_MAP, height=80, apply_pn=use_pn, cache_dir=\"cache_png\")\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_batch, num_workers=0)\n",
    "    val_loader = DataLoader(val_ds, batch_size=4, shuffle=False, collate_fn=collate_batch, num_workers=0)\n",
    "    test_loader = DataLoader(test_ds, batch_size=1, shuffle=False, collate_fn=collate_batch, num_workers=0)\n",
    "\n",
    "    train_texts = [TRANS_MAP.get(os.path.splitext(os.path.basename(p))[0], \"\") for p in train_subset]\n",
    "    stoi, itos = build_charset(train_texts)\n",
    "    num_classes = len(itos)\n",
    "    in_channels = 1 if channel_mode=='gray' else 3\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = CRNN(num_classes=num_classes, in_channels=in_channels).to(device).to(torch.float32)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "    scaler = amp.GradScaler(enabled=(device.type=='cuda'))\n",
    "\n",
    "    print(f\" Device: {device} | Model params: {sum(p.numel() for p in model.parameters())/1e6:.2f} M\")\n",
    "\n",
    "    best_val = float('inf')\n",
    "    best_epoch = -1\n",
    "    for ep in range(epochs):\n",
    "        print(f\"\\n--- Epoch {ep+1}/{epochs} ---\")\n",
    "        t0 = time.time()\n",
    "        loss = train_one_epoch(model, train_loader, optimizer, stoi, device, scaler)\n",
    "        t_elapsed = time.time() - t0\n",
    "        print(f\"Epoch {ep+1} done in {t_elapsed:.1f}s | Avg loss: {loss:.4f}\")\n",
    "\n",
    "        cer_v, wer_v = evaluate_model(model, val_loader, stoi, itos, device, dataset_type=DATA_TYPE)\n",
    "        print(f\" Validation: CER={cer_v:.4f}, WER={wer_v:.4f}\")\n",
    "\n",
    "        # scheduler uses loss/metric; use CER+WER average\n",
    "        scheduler.step(cer_v + wer_v)\n",
    "\n",
    "        # checkpoint best\n",
    "        val_metric = cer_v + wer_v\n",
    "        if val_metric < best_val:\n",
    "            best_val = val_metric\n",
    "            best_epoch = ep\n",
    "            torch.save(model.state_dict(), f\"crnn_best_epoch{ep+1}.pth\")\n",
    "            print(\"  Saved best model.\")\n",
    "\n",
    "        # early stop logic (ensure at least min_epochs)\n",
    "        if (ep - best_epoch) >= patience and ep+1 >= min_epochs:\n",
    "            print(f\"Early stopping triggered (no improvement for {patience} epochs).\")\n",
    "            break\n",
    "\n",
    "    # final test\n",
    "    cer_t, wer_t = evaluate_model(model, test_loader, stoi, itos, device, dataset_type=DATA_TYPE)\n",
    "    print(f\"\\n=== Final Test ===\\nTest CER={cer_t:.4f}, WER={wer_t:.4f}\")\n",
    "    return model, (cer_t, wer_t), (stoi, itos)\n",
    "\n",
    "\n",
    "# RUN with tuned params \n",
    "model, metrics, voc = run_experiment_timed(\n",
    "    augment_type='none',     \n",
    "    epochs=10,               \n",
    "    batch_size=8,            \n",
    "    use_pn=True,\n",
    "    channel_mode='gray',\n",
    "    fast_mode=False,         # full dataset\n",
    "    patience=3,\n",
    "    min_epochs=4\n",
    ")\n",
    "print(\"\\nTraining complete â€” metrics:\", metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8583db-d1d7-41be-a013-fdb8200e3abd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
